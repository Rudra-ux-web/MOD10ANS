{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1s_FeCnTESXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question 1: What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners.\n"
      ],
      "metadata": {
        "id": "A-B8MDEPESt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
        "\n",
        "Boosting is an ensemble learning technique in machine learning that aims to improve the performance of weak learners by combining them to form a strong predictive model. A weak learner is a model that performs only slightly better than random guessing. Boosting works by training multiple models sequentially, where each new model focuses on correcting the mistakes made by the previous models.\n",
        "\n",
        "The key idea behind boosting is to give more importance to data points that are misclassified in earlier stages. Initially, all training samples are given equal weight. After the first model is trained, the weights of incorrectly predicted samples are increased so that the next model pays more attention to these difficult cases. This process continues for several iterations, gradually improving overall performance.\n",
        "\n",
        "One of the most popular boosting algorithms is AdaBoost (Adaptive Boosting). In AdaBoost, each weak learner is assigned a weight based on its accuracy, and the final prediction is made using a weighted combination of all learners. Other advanced boosting methods include Gradient Boosting, XGBoost, and LightGBM, which optimize performance by minimizing a loss function using gradient-based techniques.\n",
        "\n",
        "Boosting improves weak learners by reducing bias and improving accuracy. Since each model learns from the errors of previous ones, the ensemble becomes better at capturing complex patterns in the data. This makes boosting highly effective in applications such as fraud detection, loan default prediction, and medical diagnosis.\n",
        "\n",
        "In conclusion, boosting is a powerful ensemble technique that transforms multiple weak learners into a strong model by training them sequentially and focusing on difficult data points. By reducing errors and improving predictive accuracy, boosting plays a crucial role in modern machine learning applications\n",
        "\n",
        "Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "\n",
        "AdaBoost and Gradient Boosting are both popular boosting algorithms used in machine learning, but they differ mainly in how models are trained and how errors are handled during the learning process. AdaBoost, which stands for Adaptive Boosting, trains models sequentially by adjusting the weights of training data points. Initially, all data points are given equal weights. After each weak learner is trained, the weights of misclassified samples are increased so that the next learner focuses more on these difficult cases. Each learner is also assigned a weight based on its accuracy, and the final prediction is made using a weighted vote or weighted sum of all learners. AdaBoost mainly focuses on correcting classification errors directly by re-weighting samples.\n",
        "\n",
        "Gradient Boosting, on the other hand, also trains models sequentially but uses a different approach based on optimization of a loss function. Instead of re-weighting data points, Gradient Boosting fits each new model to the residual errors (differences between actual and predicted values) made by the previous model. It uses gradient descent techniques to minimize the loss function, which allows it to handle both classification and regression problems more flexibly. Each new model learns how to reduce the remaining error by moving in the direction of the negative gradient of the loss function.\n",
        "\n",
        "Another important difference is that AdaBoost is more sensitive to noisy data and outliers because it increases the weights of misclassified points, which may include noise. Gradient Boosting is generally more robust and provides greater control through parameters such as learning rate, tree depth, and number of estimators.\n",
        "\n",
        "In conclusion, AdaBoost trains models by re-weighting misclassified samples and combining learners based on accuracy, while Gradient Boosting trains models by fitting to residual errors using gradient-based optimization. These differences make Gradient Boosting more flexible and powerful for complex real-world problems, while AdaBoost remains simpler and effective for cleaner datasets.\n",
        "\n",
        "\n",
        "Question 3: How does regularization help in XGBoost?\n",
        "\n",
        "Regularization in XGBoost plays a crucial role in improving model performance by preventing overfitting and ensuring better generalization on unseen data. XGBoost, which stands for Extreme Gradient Boosting, is an advanced boosting algorithm that builds decision trees sequentially while minimizing a loss function. Since boosting models can easily become too complex, regularization is used to control model complexity.\n",
        "\n",
        "XGBoost introduces regularization directly into its objective function by adding penalty terms for model complexity. These penalties include L1 regularization (Lasso) on leaf weights, L2 regularization (Ridge) on leaf weights, and a penalty on the number of leaf nodes in a tree. By penalizing large weights and overly complex trees, XGBoost discourages the model from fitting noise in the training data.\n",
        "\n",
        "Regularization helps XGBoost in several ways. It simplifies the structure of decision trees by limiting their depth and number of leaves, which reduces variance. It also stabilizes the learning process and improves prediction accuracy on test data. Additionally, regularization allows better control over bias–variance trade-off through tunable parameters, making the model more flexible for different datasets.\n",
        "\n",
        "Another advantage of regularization in XGBoost is that it improves robustness when dealing with noisy or high-dimensional data. By preventing individual trees from becoming too influential, the ensemble remains balanced and less sensitive to outliers.\n",
        "\n",
        "In conclusion, regularization in XGBoost helps prevent overfitting by controlling tree complexity and penalizing large weights. This leads to more stable, accurate, and generalizable models, making XGBoost highly effective for real-world machine learning problems\n",
        "\n",
        "\n",
        "Question 4: Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "CatBoost is considered highly efficient for handling categorical data because it is specifically designed to process categorical features directly and effectively without requiring extensive manual preprocessing. In many machine learning algorithms, categorical variables must be converted into numerical form using techniques such as one-hot encoding or label encoding, which can increase dimensionality, introduce bias, and lead to overfitting. CatBoost overcomes these limitations by using advanced encoding techniques internally.\n",
        "\n",
        "The key reason CatBoost is efficient with categorical data is its use of ordered target encoding. Instead of replacing categorical values with simple numerical labels, CatBoost encodes categories based on the target statistics while maintaining the correct order of data. This approach prevents target leakage, a common problem where information from the target variable unintentionally influences model training.\n",
        "\n",
        "CatBoost also uses a unique ordered boosting strategy, where each model is trained using only past data points. This reduces prediction bias and improves generalization, especially when categorical features have high cardinality. Additionally, CatBoost automatically handles missing values and reduces the need for extensive feature engineering.\n",
        "\n",
        "Another advantage is that CatBoost can efficiently handle datasets with a large number of categorical features and categories without significantly increasing model complexity. It also provides stable performance with minimal hyperparameter tuning, making it suitable for real-world applications such as recommendation systems, fraud detection, and financial risk modeling.\n",
        "\n",
        "In conclusion, CatBoost is efficient for handling categorical data because it natively supports categorical features, avoids target leakage through ordered encoding, reduces preprocessing effort, and delivers accurate and robust predictions. These advantages make CatBoost a powerful and practical choice for datasets containing complex categorical variables.\n",
        "\n",
        "\n",
        "Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
        "\n",
        "Boosting techniques are preferred over bagging methods in many real-world applications where high predictive accuracy and the ability to learn complex patterns are more important than simply reducing variance. Boosting works by training models sequentially and focusing on correcting previous mistakes, which makes it especially effective in problems where small improvements in prediction quality have a large impact.\n",
        "\n",
        "One major real-world application of boosting is financial risk analysis, such as loan default prediction and credit scoring. In these cases, boosting algorithms like Gradient Boosting and XGBoost are preferred because they can handle complex relationships in customer data and focus on difficult-to-classify high-risk cases. This leads to better risk assessment compared to bagging methods.\n",
        "\n",
        "Another important application is fraud detection in banking and insurance. Fraudulent transactions are rare and often difficult to detect. Boosting is well suited for such imbalanced datasets because it gives more importance to misclassified cases, improving the detection of fraud compared to bagging approaches.\n",
        "\n",
        "Boosting is also widely used in medical diagnosis, where accurate predictions are critical. For example, boosting algorithms are used in disease prediction and cancer detection because they can capture subtle patterns in medical data and improve classification accuracy. In contrast, bagging methods may miss these complex patterns.\n",
        "\n",
        "In computer vision and image recognition, boosting techniques such as AdaBoost have been successfully used in face detection and object recognition tasks. Boosting helps in combining weak classifiers to create strong models that can identify fine details in images.\n",
        "\n",
        "In conclusion, boosting techniques are preferred over bagging methods in applications that require high accuracy, handling of complex and imbalanced data, and strong predictive performance. Fields such as finance, fraud detection, healthcare, and computer vision benefit greatly from the strengths of boosting algorithms.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "liqKb1YlErKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6: Write a Python program to:\n",
        "#● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "#● Print the model accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data      # Features\n",
        "y = data.target    # Labels\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Initialize the AdaBoost Classifier\n",
        "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "# Step 4: Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"AdaBoost Classifier Accuracy: {accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jt_qlqJdHvEZ",
        "outputId": "f180173d-28e3-4599-efc8-58c2a182b1f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "#● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "#● Evaluate performance using R-squared score\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Step 1: Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data      # Features\n",
        "y = data.target    # Target values (house prices)\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Initialize the Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Step 4: Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate performance using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Gradient Boosting Regressor R-squared Score: {r2:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaFpR9dPII1J",
        "outputId": "4db795e6-bf7e-45d0-f8cb-82acbbbdf36a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R-squared Score: 0.7756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "# ● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "# ● Tune the learning rate using GridSearchCV\n",
        "# ● Print the best parameters and accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data      # Features\n",
        "y = data.target    # Labels\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Initialize the XGBoost Classifier\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Step 4: Define the parameter grid for GridSearchCV (tuning learning rate)\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Step 5: Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid,\n",
        "                           scoring='accuracy', cv=5, n_jobs=-1, verbose=1)\n",
        "\n",
        "# Step 6: Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Print the best parameters and best score (accuracy)\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Cross-validation Accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Step 8: Make predictions with the best estimator on the test set\n",
        "y_pred_tuned = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Step 9: Calculate and print the accuracy on the test set\n",
        "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
        "print(f\"XGBoost Classifier Test Accuracy (tuned learning rate): {accuracy_tuned:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DjT_5QFIssG",
        "outputId": "b4904882-a739-4a71-85fb-156e29d83646"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
            "Best Parameters: {'learning_rate': 0.2}\n",
            "Best Cross-validation Accuracy: 0.9670\n",
            "XGBoost Classifier Test Accuracy (tuned learning rate): 0.9561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [17:24:53] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9.\n",
        "# Import necessary libraries\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Initialize the CatBoost Classifier\n",
        "model = CatBoostClassifier(\n",
        "    iterations=200,      # number of trees\n",
        "    learning_rate=0.1,\n",
        "    depth=4,\n",
        "    verbose=0,           # silent mode\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 4: Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions (ensure integer labels)\n",
        "y_pred = model.predict(X_test).astype(int)\n",
        "\n",
        "# Step 6: Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"CatBoost Classifier Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Step 7: Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Step 8: Plot confusion matrix using seaborn\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - CatBoost Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wgw058faLY8K",
        "outputId": "f8828421-475b-42b2-fe5b-fb63ce9a2c3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catboost'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3440522023.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import necessary libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_breast_cancer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and categorical features.\n",
        "\n",
        "1. Data Preprocessing & Handling Missing/Categorical Values\n",
        "\n",
        "Missing values:\n",
        "\n",
        "For numerical features, fill missing values using median or mean.\n",
        "\n",
        "For categorical features, fill missing values using the mode or create a special category like \"Unknown\".\n",
        "\n",
        "Categorical features:\n",
        "\n",
        "If using CatBoost, it can directly handle categorical variables.\n",
        "\n",
        "If using XGBoost/AdaBoost, apply one-hot encoding or label encoding for categorical variables.\n",
        "\n",
        "Scaling:\n",
        "\n",
        "Not strictly necessary for boosting models, but numeric features can be standardized if needed.\n",
        "\n",
        "Handling imbalanced dataset:\n",
        "\n",
        "Use SMOTE or RandomOverSampler to balance the dataset.\n",
        "\n",
        "Alternatively, set class weights in the boosting algorithm to penalize misclassification of the minority class.\n",
        "\n",
        "2. Choice of Boosting Algorithm\n",
        "\n",
        "AdaBoost: Works well for simple datasets but can struggle with missing values and categorical variables.\n",
        "\n",
        "XGBoost: Powerful and widely used for tabular data; handles missing values automatically.\n",
        "\n",
        "CatBoost: Best choice for datasets with categorical features and missing values; requires minimal preprocessing.\n",
        "\n",
        "✅ For this dataset, I would choose CatBoost because it handles categorical features and missing values efficiently while providing high accuracy for imbalanced datasets.\n",
        "\n",
        "3. Hyperparameter Tuning Strategy\n",
        "\n",
        "Use GridSearchCV or RandomizedSearchCV for tuning:\n",
        "\n",
        "CatBoost important parameters:\n",
        "\n",
        "iterations (number of trees)\n",
        "\n",
        "learning_rate (step size for updates)\n",
        "\n",
        "depth (tree depth)\n",
        "\n",
        "l2_leaf_reg (regularization to prevent overfitting)\n",
        "\n",
        "Use 5-fold cross-validation to ensure stable results.\n",
        "\n",
        "For large datasets, RandomizedSearchCV is faster and can find good parameters efficiently.\n",
        "\n",
        "4. Evaluation Metrics\n",
        "Since the dataset is imbalanced, accuracy alone is misleading. I would use:\n",
        "\n",
        "Precision & Recall: Measure model’s ability to correctly identify loan defaulters (positive class).\n",
        "\n",
        "F1-Score: Balance between precision and recall.\n",
        "\n",
        "ROC-AUC: Measures model’s ability to separate classes.\n",
        "\n",
        "Confusion Matrix: Visualize true positives, false positives, false negatives, and true negatives.\n",
        "\n",
        "✅ These metrics help focus on minimizing false negatives (predicting non-default when the customer actually defaults), which is critical for financial risk.\n",
        "\n",
        "5. Business Benefits\n",
        "\n",
        "Risk reduction: Accurately predicting defaults allows the company to adjust lending criteria and reduce bad loans.\n",
        "\n",
        "Profit optimization: Focus resources on low-risk customers while offering tailored products.\n",
        "\n",
        "Customer insights: Understand which demographic or behavioral factors are linked to default risk.\n",
        "\n",
        "Regulatory compliance: Demonstrate that lending decisions are data-driven and fair, reducing bias.\n",
        "\n"
      ],
      "metadata": {
        "id": "DUWf0irDLoXe"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}